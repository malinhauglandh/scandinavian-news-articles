{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Scandinavian News Articles***",
   "id": "78dee509fea0b1c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook is a part of the final project in *Computational Tools for data science 02807*",
   "id": "afe2a2834a6d0f39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ***Imports***",
   "id": "d062de4525c7c6d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd \n",
    "from IPython.display import display, HTML"
   ],
   "id": "67878f2dfa1703b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ***Import Dataset***",
   "id": "8f6a4901be55d7fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The datasets have been scraped using ..",
   "id": "26c4a282136d35b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "news_articles = pd.read_csv('data/articles.csv')\n",
    "\n",
    "# information about the dataset\n",
    "news_articles.head(10)\n",
    "news_articles.shape[0]\n",
    "news_articles.info()"
   ],
   "id": "ddc811aede608a98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ***Preprocessing Steps***\n",
    "\n",
    "#### 1. Remove irrelevant links\n",
    "\n",
    "In some norwegian articles posted by \"vg.no\" there are links to another website called \"e24.no\". These are not relevant for the project, and are therefore dropped from the data frame"
   ],
   "id": "445e40ffb020ff44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# drop all norwegian articles from \"vg\" that contains \"e24\" in the url\n",
    "e24_links = news_articles[(news_articles['country'] == 'norway') & (news_articles['source'] == 'vg') & (news_articles['url'].str.contains('https://e24.no'))]\n",
    "news_articles = news_articles.drop(e24_links.index)\n",
    "news_articles.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print number of articles after preprocessing\n",
    "print(\"Number of articles after preprocessing:\", news_articles.shape[0])\n",
    "\n",
    "news_articles.head(10)\n"
   ],
   "id": "248a56db04e9d473"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2. Remove invalid source\n",
    "\n",
    "During scraping the source \"news\" has been included, which is not a valid source. Rows with this source has also been dropped. "
   ],
   "id": "1773db421ceb7d59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# count rows with source \"news\"\n",
    "invalid_source = news_articles[news_articles['source'] == 'news']\n",
    "print(\"Number of articles with source 'news':\", invalid_source.shape[0])\n",
    "\n",
    "# drop rows with source \"news\"\n",
    "news_articles = news_articles[news_articles['source'] != 'news']\n",
    "news_articles.reset_index(drop=True, inplace=True)\n",
    "print(\"Number of articles after dropping 'news' source:\", news_articles.shape[0])"
   ],
   "id": "69fe215275829edb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3. Standardize source names\n",
    "\n",
    "Some sources have uppercase letters and some not, so we standardize by only using lowercase."
   ],
   "id": "2b385dc58600b007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "news_articles['source'] = news_articles['source'].str.lower()",
   "id": "e6100e7f4771c2d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Remove articles outside the intended time period 2020-2025",
   "id": "f7f4490dda28cd4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "news_articles['date'] = pd.to_datetime(news_articles['date'], errors='coerce')\n",
    "news_articles = news_articles[(news_articles['date'] >= '2020-01-01') & (news_articles['date'] <= '2025-12-31')]\n",
    "news_articles.reset_index(drop=True, inplace=True)\n",
    "print(\"Number of articles after removing those outside 2020-2025:\", news_articles.shape[0])"
   ],
   "id": "e6003a1fcdfaa4ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ***Dataset Exploration***\n",
    "\n",
    "#### Source information\n",
    "\n",
    "The table below presents information about the sources we have retrieved. "
   ],
   "id": "fef893d8e94eb57c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_sources = news_articles[['country', 'source']].drop_duplicates().sort_values(by='country')\n",
    "\n",
    "sources_per_country = unique_sources['country'].value_counts()\n",
    "display(HTML(\"<h4 style='font-weight:700'>Number of Sources per Country:</h4>\"))\n",
    "display(HTML(\"<pre style='font-family:monospace'>{}</pre>\".format(\n",
    "    sources_per_country.rename(index=lambda s: s.capitalize()).to_string(header=False)\n",
    ")))\n",
    "\n",
    "src_counts = (news_articles\n",
    "              .groupby(['country', 'source'])\n",
    "              .size()\n",
    "              .reset_index(name='n_articles'))\n",
    "\n",
    "# compute average worldview_score per country and source\n",
    "temp_view = news_articles.copy()\n",
    "temp_view['worldview_num'] = pd.to_numeric(temp_view['worldview_score'], errors='coerce')\n",
    "avg_world = temp_view.groupby(['country', 'source'])['worldview_num'].mean().reset_index(name='avg_worldview').round(2)\n",
    "display_df = src_counts.merge(avg_world, on=['country', 'source'], how='left')\n",
    "\n",
    "# add column showing the time range of articles per country and source\n",
    "time_ranges = (news_articles\n",
    "                .groupby(['country', 'source'])\n",
    "                .agg(first_article=('date', 'min'), last_article=('date', 'max'))\n",
    "                .reset_index())\n",
    "\n",
    "time_ranges['time_range'] = (\n",
    "    time_ranges['first_article'].dt.strftime('%Y-%m-%d') +\n",
    "    ' to ' +\n",
    "    time_ranges['last_article'].dt.strftime('%Y-%m-%d')\n",
    ")\n",
    "\n",
    "display_df = display_df.merge(time_ranges[['country', 'source', 'time_range']], on=['country', 'source'])  \n",
    "display_df = display_df[['source', 'n_articles', 'avg_worldview', 'country', 'time_range']]\n",
    "display_df = display_df.sort_values(by='country').copy()\n",
    "display(HTML(display_df.to_html(index=False, border=0)))\n"
   ],
   "id": "f3926dc38808380c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Article Distribution",
   "id": "8e345c5dc8ef242c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "countries = news_articles['country'].unique()\n",
    "num_countries = len(countries)\n",
    "\n",
    "fig, axes = plt.subplots(1, num_countries, figsize=(18, 6), sharey=True)\n",
    "\n",
    "if num_countries == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, country in zip(axes, countries):\n",
    "    country_data = news_articles[news_articles['country'] == country].copy()\n",
    "\n",
    "    for i, source in enumerate(country_data['source'].unique()):\n",
    "        source_data = country_data[country_data['source'] == source].copy()\n",
    "        source_data['year'] = source_data['date'].dt.year\n",
    "        year_counts = source_data['year'].value_counts().sort_index()\n",
    "\n",
    "        ax.bar(\n",
    "            year_counts.index + (0.1 * i),\n",
    "            year_counts.values,\n",
    "            width=0.1,\n",
    "            label=source\n",
    "        )\n",
    "\n",
    "    ax.set_title(f'Articles per Year – {country.capitalize()}')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_xticks(range(2020, 2026))\n",
    "    ax.legend()\n",
    "\n",
    "axes[0].set_ylabel('Number of Articles')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "a3ff1d94787f7ba9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Average Worldview in each Country from 2020-2025",
   "id": "4139fef3d623115e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "worldview_over_time = (news_articles\n",
    "                       .copy())\n",
    "worldview_over_time['worldview_num'] = pd.to_numeric(worldview_over_time['worldview_score'], errors='coerce')\n",
    "worldview_over_time['year'] = worldview_over_time['date'].dt.year\n",
    "avg_worldview_time = (worldview_over_time\n",
    "                      .groupby(['country', 'year'])['worldview_num']\n",
    "                      .mean()\n",
    "                      .reset_index())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for country in avg_worldview_time['country'].unique():\n",
    "    country_data = avg_worldview_time[avg_worldview_time['country'] == country]\n",
    "    plt.plot(country_data['year'], country_data['worldview_num'], marker='o', label=country.capitalize())\n",
    "\n",
    "    \n",
    "plt.title('Average Worldview Score Over Time by Country')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Worldview Score')\n",
    "plt.xticks(avg_worldview_time['year'].unique())\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "4df35201935abc75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ***TF-IDF***",
   "id": "fad6371039911629"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "base_stopwords = set(stopwords.words('english') +\n",
    "                       stopwords.words('swedish') +\n",
    "                       stopwords.words('danish') +\n",
    "                       stopwords.words('norwegian'))\n",
    "\n",
    "# Domain-specific extra stopwords: outlets, generic news words, light verbs/adverbs\n",
    "extra_stopwords = {\n",
    "    \"vg\", \"nrk\", \"aftonbladet\", \"dr\", \"artikler\", \"artikkel\", \"artikel\",\n",
    "    \"sier\", \"mener\", \"siste\", \"litt\", \"mye\", \"mer\", \"år\", \"dag\", \"dager\",\n",
    "    \"blant\", \"andre\", \"må\", \"fikk\", \"veldig\", \"annet\", \"går\", \"nå\",\n",
    "    \"uten\", \"over\", \"etter\", \"inn\", \"to\", \"via\", \"se\", \"gå\", \"kom\", \"kommet\",\n",
    "    \"ifølge\", \"har\", \"ha\", \"være\", \"viktig\", \"nyhet\", \"nyheter\", \"les\", \"lese\",\n",
    "    # add more as you notice junk terms\n",
    "}\n",
    "\n",
    "custom_stopwords = list(base_stopwords.union(extra_stopwords))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stopwords,   # must be list, not set\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "news_articles['text'] = news_articles['headline'].fillna('') + \" \" + news_articles['content'].fillna('')\n",
    "tfidf_matrix = vectorizer.fit_transform(news_articles['text'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(\"TF-IDF shape:\", tfidf_matrix.shape)\n"
   ],
   "id": "33ac6f0a83c2d86f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def top_tfidf_terms(mask, n=30):\n",
    "    sub = tfidf_matrix[mask]\n",
    "    mean_scores = sub.mean(axis=0).A1\n",
    "    top_idx = mean_scores.argsort()[::-1][:n]\n",
    "    return [(feature_names[i], mean_scores[i]) for i in top_idx]\n",
    "\n",
    "# Example: top terms for Sweden\n",
    "norway_mask = news_articles['country'] == 'norway'\n",
    "top_tfidf_terms(norway_mask)\n"
   ],
   "id": "9ff6e0b25f6233fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
