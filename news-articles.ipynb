{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Scandinavian News Articles***"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook is a part of the final project in ***Computational Tools for data science 02807*** and is written by *group 31* consisting of the members:\n",
    "\n",
    "Felix Emil Bruun \\\n",
    "Eline Evje \\\n",
    "Malin Haugland Høli \\\n",
    "Ina Martini\n",
    "\n",
    "Deadline: December 6, 2025\n",
    "\n",
    "The main objective of the project is to analyze and compare political articles from various Scandinavian news outlets using machine learning, data mining techniques, and article classification."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ***Imports***"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "from datasketch import MinHashLSH, MinHash\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ***Import Dataset***"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The datasets have been scraped using Selenium and BeautifulSoup."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "news_articles = pd.read_csv('data/articles.csv')\n",
    "\n",
    "# information about the dataset\n",
    "news_articles.shape[0]\n",
    "news_articles.info()\n",
    "news_articles.head(5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ***1. Preprocessing Steps***\n",
    "\n",
    "### 1.1 Cleaning subscription boilerplate and layout noise"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Many scraped news articles contain subscription prompts, “read more” links, and layout fragments that are not part of the actual content.\n",
    "\n",
    "To avoid polluting the later similarity and clustering analyses, this has been removed from the `content` column. The resulting `content_clean` column is used for further text processing."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) Define \"boilerplate\" markers you want to cut away and everything after\n",
    "SUBSCRIPTION_MARKERS = [\n",
    "    \"Allerede abonnent? Log ind\",\n",
    "    \"Allerede abonnent? Log ind her\",\n",
    "    \"Log ind for at læse\",\n",
    "    \"Log ind for at læse hele artiklen\",\n",
    "    \"Log ind for at læse videre\",\n",
    "    \"Logg inn for å lese\",\n",
    "    \"Bli abonnent\",\n",
    "    \"Tegn abonnement\",\n",
    "    \"Få adgang til hele artiklen\",\n",
    "    \"Få adgang til alt indhold\",\n",
    "\n",
    "    # Swedish / Norwegian / Danish \"read more\" prompts\n",
    "    \"Läs mer\",       # SE\n",
    "    \"Læs mere\",      # DK\n",
    "    \"Les mer\",       # NO\n",
    "\n",
    "    # DN print-view header\n",
    "    \"En utskrift från Dagens Nyheter\",\n",
    "]\n",
    "\n",
    "def clean_content(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    t = str(text)\n",
    "\n",
    "    # --- basic formatting fixes ---\n",
    "    # remove leading \"/ \" like \"/ Under 30 minuter ...\"\n",
    "    t = re.sub(r'^\\s*/\\s*', '', t)\n",
    "\n",
    "    # remove bullet characters that often appear in scraped content\n",
    "    t = t.replace(\"•\", \" \")\n",
    "\n",
    "    # --- cut off subscription / boilerplate tail ---\n",
    "    lower = t.lower()\n",
    "    cut_pos = len(t)\n",
    "    for marker in SUBSCRIPTION_MARKERS:\n",
    "        idx = lower.find(marker.lower())\n",
    "        if idx != -1 and idx < cut_pos:\n",
    "            cut_pos = idx\n",
    "\n",
    "    t = t[:cut_pos]\n",
    "\n",
    "    # --- normalize whitespace ---\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "\n",
    "    return t\n",
    "\n",
    "# Apply to your dataframe\n",
    "news_articles['content_clean'] = news_articles['content'].apply(clean_content)\n",
    "\n",
    "news_articles.head(5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Remove irrelevant links\n",
    "\n",
    "In some norwegian articles posted by \"vg.no\" there are links to another website called \"e24.no\". These are not relevant for the project, and are therefore dropped from the data frame"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# drop all norwegian articles from \"vg\" that contains \"e24\" in the url\n",
    "e24_links = news_articles[(news_articles['country'] == 'norway') & (news_articles['source'] == 'vg') & (news_articles['url'].str.contains('https://e24.no'))]\n",
    "news_articles = news_articles.drop(e24_links.index)\n",
    "news_articles.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print number of articles after preprocessing\n",
    "print(\"Number of articles after preprocessing:\", news_articles.shape[0])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Remove invalid source\n",
    "\n",
    "During scraping the source \"news\" has been included, which is not a valid source. Rows with this source has also been dropped. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# count rows with source \"news\"\n",
    "invalid_source = news_articles[news_articles['source'] == 'news']\n",
    "print(\"Number of articles with source 'news':\", invalid_source.shape[0])\n",
    "\n",
    "# drop rows with source \"news\"\n",
    "news_articles = news_articles[news_articles['source'] != 'news']\n",
    "news_articles.reset_index(drop=True, inplace=True)\n",
    "print(\"Number of articles after dropping 'news' source:\", news_articles.shape[0])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.4 Standardize source names\n",
    "\n",
    "Some sources have uppercase letters and some not, so we standardize by only using lowercase."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "news_articles['source'] = news_articles['source'].str.lower()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5 Remove articles outside the intended time period 2020-2025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "news_articles['date'] = pd.to_datetime(news_articles['date'], errors='coerce')\n",
    "news_articles = news_articles[(news_articles['date'] >= '2020-01-01') & (news_articles['date'] <= '2025-12-31')]\n",
    "news_articles.reset_index(drop=True, inplace=True)\n",
    "print(\"Number of articles after removing those outside 2020-2025:\", news_articles.shape[0])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.6 Remove duplicates"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Drop strict duplicates (same URL + same content + same headline)\n",
    "news_articles = news_articles.drop_duplicates(\n",
    "    subset=['url', 'source', 'content_clean', 'headline']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"After strict duplicates:\", news_articles.shape[0])\n",
    "\n",
    "# 2. Drop soft duplicates (same content/headline but different URL)\n",
    "news_articles = news_articles.drop_duplicates(\n",
    "    subset=['content_clean', 'headline']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"After soft duplicates:\", news_articles.shape[0])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.7 Drop articles with worldview score 6, i.e., not classified"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "news_articles = news_articles[news_articles['worldview_score'] != 6]\n",
    "news_articles.reset_index(drop=True, inplace=True)\n",
    "print(\"Number of articles after dropping unclassified worldview score:\", news_articles.shape[0])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ***2. Dataset Exploration***\n",
    "\n",
    "### 2.1 Source information\n",
    "\n",
    "The table below presents information about the *sources* (news outlets) that has been retrieved. There are 6 sources from Sweden, 5 from Denmark and 4 from Norway. The dataset for the analysis includes articles from 2020-2025. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_sources = news_articles[['country', 'source']].drop_duplicates().sort_values(by='country')\n",
    "\n",
    "sources_per_country = unique_sources['country'].value_counts()\n",
    "display(HTML(\"<h4 style='font-weight:700'>Number of Sources per Country:</h4>\"))\n",
    "display(HTML(\"<pre style='font-family:monospace'>{}</pre>\".format(\n",
    "    sources_per_country.rename(index=lambda s: s.capitalize()).to_string(header=False)\n",
    ")))\n",
    "\n",
    "src_counts = (news_articles\n",
    "              .groupby(['country', 'source'])\n",
    "              .size()\n",
    "              .reset_index(name='n_articles'))\n",
    "\n",
    "# compute average worldview_score per country and source\n",
    "temp_view = news_articles.copy()\n",
    "temp_view['worldview_num'] = pd.to_numeric(temp_view['worldview_score'], errors='coerce')\n",
    "avg_world = temp_view.groupby(['country', 'source'])['worldview_num'].mean().reset_index(name='avg_worldview').round(2)\n",
    "display_df = src_counts.merge(avg_world, on=['country', 'source'], how='left')\n",
    "\n",
    "# add column showing the time range of articles per country and source\n",
    "time_ranges = (news_articles\n",
    "                .groupby(['country', 'source'])\n",
    "                .agg(first_article=('date', 'min'), last_article=('date', 'max'))\n",
    "                .reset_index())\n",
    "\n",
    "time_ranges['time_range'] = (\n",
    "    time_ranges['first_article'].dt.strftime('%Y-%m-%d') +\n",
    "    ' to ' +\n",
    "    time_ranges['last_article'].dt.strftime('%Y-%m-%d')\n",
    ")\n",
    "\n",
    "display_df = display_df.merge(time_ranges[['country', 'source', 'time_range']], on=['country', 'source'])  \n",
    "display_df = display_df[['source', 'n_articles', 'avg_worldview', 'country', 'time_range']]\n",
    "display_df = display_df.sort_values(by='country').copy()\n",
    "display(HTML(display_df.to_html(index=False, border=0)))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Article distribution per year in the different countries"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "countries = news_articles['country'].unique()\n",
    "num_countries = len(countries)\n",
    "\n",
    "fig, axes = plt.subplots(1, num_countries, figsize=(18, 6), sharey=True)\n",
    "\n",
    "if num_countries == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, country in zip(axes, countries):\n",
    "    country_data = news_articles[news_articles['country'] == country].copy()\n",
    "\n",
    "    for i, source in enumerate(country_data['source'].unique()):\n",
    "        source_data = country_data[country_data['source'] == source].copy()\n",
    "        source_data['year'] = source_data['date'].dt.year\n",
    "        year_counts = source_data['year'].value_counts().sort_index()\n",
    "\n",
    "        ax.bar(\n",
    "            year_counts.index + (0.1 * i),\n",
    "            year_counts.values,\n",
    "            width=0.1,\n",
    "            label=source\n",
    "        )\n",
    "\n",
    "    ax.set_title(f'Articles per Year – {country.capitalize()}')\n",
    "    ax.set_xlabel('Year')\n",
    "    plt.figtext(0.5, -0.01, 'Figure 1: Shows the distribution of articles by year and source. Norway has the least amount of articles.', ha='center', fontsize=10)\n",
    "    ax.set_xticks(range(2020, 2026))\n",
    "    ax.legend()\n",
    "\n",
    "axes[0].set_ylabel('Number of Articles')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 Average worldview in each country from 2020-2025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "worldview_over_time = (news_articles\n",
    "                       .copy())\n",
    "worldview_over_time['worldview_num'] = pd.to_numeric(worldview_over_time['worldview_score'], errors='coerce')\n",
    "worldview_over_time['year'] = worldview_over_time['date'].dt.year\n",
    "avg_worldview_time = (worldview_over_time\n",
    "                      .groupby(['country', 'year'])['worldview_num']\n",
    "                      .mean()\n",
    "                      .reset_index())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for country in avg_worldview_time['country'].unique():\n",
    "    country_data = avg_worldview_time[avg_worldview_time['country'] == country]\n",
    "    plt.plot(country_data['year'], country_data['worldview_num'], marker='o', label=country.capitalize())\n",
    "\n",
    "    \n",
    "plt.title('Average Worldview Score Over Time by Country')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Worldview Score')\n",
    "plt.figtext(0.5, 0.01, 'Figure 2: Illustrates the average worldview score over time by country based on the news articles.', ha='center', fontsize=10)\n",
    "plt.xticks(avg_worldview_time['year'].unique())\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ***3. Data Analysis***"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Implementing multilingual embeddings\n",
    "\n",
    "Before comparing similarity methods (LSH, ANN, and clustering), a transformers model was used to map the different languages to a shared space. The result is is a embedding matrix that will be used for further analysis."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use headline + cleaned content for richer semantics\n",
    "texts = (\n",
    "    news_articles['headline'].fillna('') + ' ' +\n",
    "    news_articles['content_clean'].fillna('')\n",
    ").astype(str).tolist()\n",
    "\n",
    "# Multilingual model that maps different languages to a shared space\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embed_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Dense embedding matrix: shape (n_docs, embedding_dim)\n",
    "emb_matrix = embed_model.encode(\n",
    "    texts,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=64  # tweak if GPU/CPU memory is small\n",
    ")\n",
    "\n",
    "print(\"Embedding matrix shape:\", emb_matrix.shape)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Approximate Nearest Neighbor (ANN)\n",
    "\n",
    "A ANN uses a NearestNeighbors index over the TF-IDF vectors to quickly retrieve the articles most similar to a given one. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_ann_index(matrix, n_neighbors=10, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Build an ANN index using scikit-learn's NearestNeighbors.\n",
    "    Works with TF-IDF or any sparse matrix.\n",
    "    \"\"\"\n",
    "    nn = NearestNeighbors(\n",
    "        n_neighbors=n_neighbors + 1,  \n",
    "        metric=metric,\n",
    "        algorithm='brute'  # sparse matrices\n",
    "    )\n",
    "    nn.fit(matrix)\n",
    "    return nn\n",
    "\n",
    "# Build ANN index on embeddings\n",
    "ann_index = build_ann_index(emb_matrix, n_neighbors=10)\n",
    "\n",
    "def find_similar_articles(query_idx, ann_index, matrix, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    Find most similar articles to a given article using ANN.\n",
    "    Returns indices and distances of similar articles.\n",
    "    \"\"\"\n",
    "    query_vector = matrix[query_idx:query_idx+1]\n",
    "    distances, indices = ann_index.kneighbors(query_vector)\n",
    "    \n",
    "    # Remove the query article itself (first result)\n",
    "    return indices[0][1:], distances[0][1:]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def print_similar_articles(query_idx, ann_index, matrix, news_df, n):\n",
    "    # Find ANN neighbors\n",
    "    indices, distances = find_similar_articles(query_idx, ann_index, matrix, n_neighbors=n)\n",
    "\n",
    "    # Build table\n",
    "    rows = []\n",
    "    news_df.loc[query_idx, 'country']\n",
    "\n",
    "    for idx, dist in zip(indices, distances):\n",
    "        rows.append({\n",
    "            'similarity': round(1 - dist, 3),\n",
    "            'country': news_df.loc[idx, 'country'],\n",
    "            'source': news_df.loc[idx, 'source'],\n",
    "            'headline': news_df.loc[idx, 'headline'],\n",
    "            'clean_content': news_df.loc[idx, 'content_clean'],\n",
    "            'worldview_score': news_df.loc[idx, 'worldview_score']\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Example using article 5 as query\n",
    "    display(HTML(f\"<h4 style='font-weight:700'>Example article {query_idx}:</h4>\"))\n",
    "    print(f\"Headline: {news_df.loc[query_idx, 'headline']}\")\n",
    "    print(f\"Country: {news_df.loc[query_idx, 'country']} | Source: {news_df.loc[query_idx, 'source']}\")\n",
    "    print(f\"Worldview Score: {news_df.loc[query_idx, 'worldview_score']}\\n\")\n",
    "\n",
    "    display(HTML(\"<h4 style='font-weight:700'>Most similar articles:</h4>\"))\n",
    "    display(df.head(n))\n",
    "\n",
    "print_similar_articles(280, ann_index, emb_matrix, news_articles, n=10)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ANN Analysis by Country and Worldview\n",
    "\n",
    "This analysis checks how often each article’s most semantically similar neighbors come from the same country or share the same worldview score.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_similarities_by_country(ann_index, matrix, news_df, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    Analyze how similar articles cluster by country and worldview.\n",
    "    \"\"\"\n",
    "    country_similarities = []\n",
    "    worldview_similarities = []\n",
    "    \n",
    "    for i in range(len(news_df)):\n",
    "        similar_indices, distances = find_similar_articles(i, ann_index, matrix, n_neighbors)\n",
    "        \n",
    "        query_country = news_df.loc[i, 'country']\n",
    "        query_worldview = news_df.loc[i, 'worldview_score']\n",
    "        \n",
    "        # Count same-country neighbors\n",
    "        same_country = sum(1 for idx in similar_indices \n",
    "                          if news_df.loc[idx, 'country'] == query_country)\n",
    "        country_similarities.append(same_country / len(similar_indices))\n",
    "        \n",
    "        # Count same-worldview neighbors\n",
    "        same_worldview = sum(1 for idx in similar_indices \n",
    "                           if news_df.loc[idx, 'worldview_score'] == query_worldview)\n",
    "        worldview_similarities.append(same_worldview / len(similar_indices))\n",
    "    \n",
    "    return country_similarities, worldview_similarities\n",
    "\n",
    "country_sim, worldview_sim = analyze_similarities_by_country(ann_index, emb_matrix, news_articles)\n",
    "\n",
    "# Breakdown by country\n",
    "country_breakdown = news_articles.copy()\n",
    "country_breakdown['country_sim_ratio'] = country_sim\n",
    "country_breakdown['worldview_sim_ratio'] = worldview_sim\n",
    "\n",
    "country_stats = (country_breakdown\n",
    "                .groupby('country')\n",
    "                .agg({\n",
    "                    'country_sim_ratio': 'mean',\n",
    "                    'worldview_sim_ratio': 'mean'\n",
    "                })\n",
    "                .round(3))\n",
    "\n",
    "country_stats_plot = country_stats.reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=False)\n",
    "\n",
    "# --- Left bar plot: Same-country ratio ---\n",
    "axes[0].bar(country_stats_plot['country'], country_stats_plot['country_sim_ratio'])\n",
    "axes[0].set_title(\"Same-Country Neighbor Ratio\")\n",
    "axes[0].set_xlabel(\"Country\")\n",
    "axes[0].set_ylabel(\"Ratio\")\n",
    "\n",
    "# --- Right bar plot: Same-worldview ratio ---\n",
    "axes[1].bar(country_stats_plot['country'], country_stats_plot['worldview_sim_ratio'])\n",
    "axes[1].set_title(\"Same-Worldview Neighbor Ratio\")\n",
    "axes[1].set_xlabel(\"Country\")\n",
    "\n",
    "plt.figtext(0.5, -0.01, 'Figure 3: Compares the average ratios of same-country and same-worldview neighbors found via ANN similarity search across the different countries.', ha='center', fontsize=10)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "plt.show()\n",
    "\n",
    "display(HTML(\"<h4>Similarity Analysis by Country</h4>\"))\n",
    "display(HTML(country_stats.to_html()))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Measure ideological polarization within each media outlet\n",
    "The polarization score reflects how semantically similar articles within a news outlet tend to share the same worldview rating, where higher values indicate stronger ideological alignment and lower values suggest greater ideological diversity within the outlet’s content."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def outlet_polarization(df, ann_index, matrix, n=10):\n",
    "    scores = []\n",
    "    for i in range(len(df)):\n",
    "        sim_idx, _ = find_similar_articles(i, ann_index, matrix, n)\n",
    "        target = df.loc[i, 'worldview_score']\n",
    "        same = sum(df.loc[idx, 'worldview_score'] == target for idx in sim_idx)\n",
    "        scores.append(same / len(sim_idx))\n",
    "    df['polarization'] = scores\n",
    "\n",
    "    # sort by values (highest polarization first)\n",
    "    return df.groupby('source')['polarization'].mean().round(3).sort_values(ascending=True)\n",
    "\n",
    "outlet_polarization(news_articles, ann_index, tfidf_matrix)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Semantic Ideological Consistency\n",
    "The polarization score shows how semantically similar articles within a country tend to share the same worldview rating, where higher values indicate stronger ideological alignment.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def country_polarization(df, matrix, ann_index, n=10):\n",
    "    scores = []\n",
    "    for i in range(len(df)):\n",
    "        sim_idx, _ = find_similar_articles(i, ann_index, matrix, n)\n",
    "        target = df.loc[i, 'worldview_score']\n",
    "        same = sum(df.loc[idx, 'worldview_score'] == target for idx in sim_idx)\n",
    "        scores.append(same / len(sim_idx))\n",
    "\n",
    "    df['polarization'] = scores\n",
    "    return df.groupby('country')['polarization'].mean().sort_values().round(3)\n",
    "\n",
    "country_polarization(news_articles, tfidf_matrix, ann_index)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 DBSCAN Clustering"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Clustering can detect *groups of similar news articles* that emerge naturally from their content.\n",
    "\n",
    "By clustering the articles and then inspecting the cluster contents, it is possible to e.g explore whether articles with similar ***worldview score*** appear in the same clusters. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "docs = news_articles['content_clean'].astype(str).tolist()\n",
    "\n",
    "vect = CountVectorizer(analyzer='char', ngram_range=(5,5), binary=True)\n",
    "X = vect.fit_transform(docs)\n",
    "\n",
    "# Compute pairwise Jaccard distances (1 - similarity)\n",
    "distances = pdist(X.toarray(), metric=\"jaccard\")\n",
    "\n",
    "# Convert to a matrix\n",
    "dist_matrix = squareform(distances)\n",
    "\n",
    "# Convert to similarity:\n",
    "sim_matrix = 1 - dist_matrix\n",
    "\n",
    "db = DBSCAN(\n",
    "    eps=0.905,     \n",
    "    min_samples=7,\n",
    "    metric='precomputed'\n",
    ")\n",
    "\n",
    "labels = db.fit_predict(dist_matrix)\n",
    "\n",
    "cluster_counts = pd.Series(labels).value_counts().sort_index()\n",
    "print(cluster_counts)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def perform_dbscan_clustering(matrix, eps=0.3, min_samples=5, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on the feature matrix.\n",
    "    For cosine metric, we compute distances and use precomputed.\n",
    "    \"\"\"\n",
    "    if metric == 'cosine':\n",
    "        # Convert sparse matrix to dense for cosine distance calculation\n",
    "        dense_matrix = matrix.toarray() if hasattr(matrix, 'toarray') else matrix\n",
    "        distance_matrix = cosine_distances(dense_matrix)\n",
    "        \n",
    "        dbscan = DBSCAN(\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            metric='precomputed'\n",
    "        )\n",
    "        labels = dbscan.fit_predict(distance_matrix)\n",
    "    else:\n",
    "        dbscan = DBSCAN(\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            metric=metric\n",
    "        )\n",
    "        labels = dbscan.fit_predict(matrix.toarray() if hasattr(matrix, 'toarray') else matrix)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def analyze_clusters(labels, news_df):\n",
    "    \"\"\"\n",
    "    Analyze the clustering results.\n",
    "    \"\"\"\n",
    "    cluster_df = news_df.copy()\n",
    "    cluster_df['cluster'] = labels\n",
    "    \n",
    "    # Count clusters\n",
    "    cluster_counts = pd.Series(labels).value_counts().sort_index()\n",
    "    n_clusters = len(cluster_counts[cluster_counts.index != -1])\n",
    "    n_noise = cluster_counts.get(-1, 0)\n",
    "    \n",
    "    print(f\"Number of clusters: {n_clusters}\")\n",
    "    print(f\"Number of noise points: {n_noise}\")\n",
    "    print(f\"Clustering ratio: {(len(labels) - n_noise) / len(labels):.3f}\")\n",
    "    \n",
    "    return cluster_df, cluster_counts\n",
    "\n",
    "# Test different eps values\n",
    "eps_values = [0.2, 0.3, 0.4, 0.5]\n",
    "clustering_results = {}\n",
    "\n",
    "for eps in eps_values:\n",
    "    labels = perform_dbscan_clustering(tfidf_matrix, eps=eps, min_samples=3)\n",
    "    cluster_df, cluster_counts = analyze_clusters(labels, news_articles)\n",
    "    clustering_results[eps] = (labels, cluster_df, cluster_counts)\n",
    "    \n",
    "    print(f\"\\nEps={eps}:\")\n",
    "    n_clusters = len(cluster_counts[cluster_counts.index != -1])\n",
    "    n_noise = cluster_counts.get(-1, 0)\n",
    "    print(f\"  Clusters: {n_clusters}, Noise: {n_noise}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_clustering_results(clustering_results):\n",
    "    \"\"\"\n",
    "    Visualize DBSCAN clustering results for different eps values.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (eps, (labels, cluster_df, cluster_counts)) in enumerate(clustering_results.items()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot cluster sizes (excluding noise)\n",
    "        valid_clusters = cluster_counts[cluster_counts.index != -1]\n",
    "        \n",
    "        if len(valid_clusters) > 0:\n",
    "            ax.bar(range(len(valid_clusters)), valid_clusters.values)\n",
    "            ax.set_title(f'Eps={eps}: {len(valid_clusters)} clusters, {cluster_counts.get(-1, 0)} noise')\n",
    "            ax.set_xlabel('Cluster ID')\n",
    "            ax.set_ylabel('Number of Articles')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'No clusters found\\n(eps={eps})', \n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(f'Eps={eps}: No clusters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.figtext(0.5, 0.01, 'Figure 3: DBSCAN clustering results for different eps values.', ha='center', fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "visualize_clustering_results(clustering_results)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def summarize_clustering_results(clustering_results):\n",
    "    \"\"\"\n",
    "    Summarize DBSCAN results across eps values:\n",
    "    - number of clusters (excluding noise)\n",
    "    - fraction of points labeled as noise\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for eps, (labels, cluster_df, cluster_counts) in clustering_results.items():\n",
    "        total = len(labels)\n",
    "        n_noise = cluster_counts.get(-1, 0)\n",
    "        n_clusters = (cluster_counts.index != -1).sum()\n",
    "        noise_ratio = n_noise / total if total > 0 else 0.0\n",
    "        rows.append((eps, n_clusters, noise_ratio))\n",
    "\n",
    "    rows = sorted(rows, key=lambda x: x[0])\n",
    "    eps_values = [r[0] for r in rows]\n",
    "    n_clusters = [r[1] for r in rows]\n",
    "    noise_ratios = [r[2] for r in rows]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # Bar: number of clusters\n",
    "    x = np.arange(len(eps_values))\n",
    "    ax1.bar(x, n_clusters)\n",
    "    ax1.set_xlabel(\"eps\")\n",
    "    ax1.set_ylabel(\"Number of clusters\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(eps_values)\n",
    "\n",
    "    # Line: fraction of noise points\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, noise_ratios, marker='o')\n",
    "    ax2.set_ylabel(\"Fraction of noise points\")\n",
    "\n",
    "    plt.title(\"DBSCAN behavior across eps values\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# usage\n",
    "summarize_clustering_results(clustering_results)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_clusters_2d(emb_matrix, labels, title=\"Cluster Visualization (2D UMAP)\"):\n",
    "    \"\"\"\n",
    "    Project embedding matrix to 2D and color points by cluster label.\n",
    "    Noise points (-1) get their own color (usually black/gray).\n",
    "    \"\"\"\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    emb_2d = reducer.fit_transform(emb_matrix)\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    for lab, col in zip(unique_labels, colors):\n",
    "        mask = labels == lab\n",
    "        if lab == -1:\n",
    "            # Noise points\n",
    "            plt.scatter(\n",
    "                emb_2d[mask, 0], emb_2d[mask, 1],\n",
    "                c=\"lightgrey\", s=12, label=\"Noise (-1)\", alpha=0.6\n",
    "            )\n",
    "        else:\n",
    "            plt.scatter(\n",
    "                emb_2d[mask, 0], emb_2d[mask, 1],\n",
    "                c=[col], s=20, label=f\"Cluster {lab}\"\n",
    "            )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.legend(loc=\"best\", markerscale=1.5, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example: visualize UMAP clusters for eps = 0.9\n",
    "eps = 0.9\n",
    "labels, cluster_df, cluster_counts = clustering_results[eps]\n",
    "\n",
    "visualize_clusters_2d(emb_matrix, labels, title=f\"DBSCAN Clusters (eps={eps})\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Examine cluster content\n",
    "\n",
    "To interpret the DBSCAN clusters, the function below prints:\n",
    "\n",
    "- The number of articles in the cluster.\n",
    "- The distribution of countries, sources and worldview scores.\n",
    "- A small sample of article headlines and the first part of their content. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def examine_cluster_content(cluster_df, cluster_id, max_articles=5):\n",
    "    \"\"\"\n",
    "    Examine the content of a specific cluster.\n",
    "    \"\"\"\n",
    "    cluster_articles = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_articles)} articles):\")\n",
    "    print(\"Country distribution:\", cluster_articles['country'].value_counts().to_dict())\n",
    "    print(\"Source distribution:\", cluster_articles['source'].value_counts().to_dict())\n",
    "    print(\"Worldview distribution:\", cluster_articles['worldview_score'].value_counts().to_dict())\n",
    "    \n",
    "    print(f\"\\nSample articles (showing up to {max_articles}):\")\n",
    "    for i, (idx, row) in enumerate(cluster_articles.head(max_articles).iterrows()):\n",
    "        print(f\"{i+1}. [{row['source']}] {row['headline']}\")\n",
    "        print(f\"   Content: {row['content_clean'][:100]}...\\n\")\n",
    "\n",
    "# Choose best eps value and examine clusters\n",
    "best_eps = 0.3  # adjust based on your results\n",
    "_, best_cluster_df, best_cluster_counts = clustering_results[best_eps]\n",
    "\n",
    "# Show largest clusters\n",
    "valid_clusters = best_cluster_counts[best_cluster_counts.index != -1]\n",
    "largest_clusters = valid_clusters.nlargest(3)\n",
    "\n",
    "for cluster_id in largest_clusters.index:\n",
    "    examine_cluster_content(best_cluster_df, cluster_id)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.4 Locality Sensitive Hashing"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The LSH implementation below builds a *simple MinHash + LSH pipeline* as a baseline for detecting near-duplicate articles.\n",
    "\n",
    "The main steps are:\n",
    "\n",
    "1. MinHash signatures  \n",
    "   Each article is represented by a MinHash object built from word shingles.  \n",
    "   These signatures approximate the Jaccard similarity between articles.\n",
    "\n",
    "2. MinHashLSH index \n",
    "   Each MinHash is inserted into a `MinHashLSH` index with a chosen similarity threshold.  \n",
    "   This makes it possible to query similar articles without having to check all pairs. \n",
    "\n",
    "3. Enumerating similar pairs\n",
    "   The implementation loops over all articles, queries the LSH index for neighbors, and collects all unique pairs of articles that LSH thinks are similar.\n",
    "\n",
    "The headline and content is combined in order to capture the most meaning of an article. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "news_articles['combined'] = news_articles['headline'].fillna('') + ' ' + news_articles['content_clean'].fillna('')\n",
    "\n",
    "def get_shingles(text, k=5):\n",
    "    \"\"\"\n",
    "    Create word shingles (k-grams) from a text.\n",
    "    Example: k=5 means sequences of 5 consecutive words.\n",
    "    \"\"\"\n",
    "    # basic tokenization: keep alphanumeric words\n",
    "    tokens = re.findall(r'\\w+', str(text).lower())\n",
    "    if len(tokens) < k:\n",
    "        return set([' '.join(tokens)]) if tokens else set()\n",
    "    return {\n",
    "        ' '.join(tokens[i:i + k])\n",
    "        for i in range(len(tokens) - k + 1)\n",
    "    }\n",
    "\n",
    "# Quick test on one article:\n",
    "example_shingles = get_shingles(news_articles['combined'].iloc[3], k=5)\n",
    "print(\"Number of shingles:\", len(example_shingles)) \n",
    "print(\"The first 5 shingles:\", list(example_shingles)[:5])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_minhash(shingles, num_perm=128):\n",
    "    \"\"\"\n",
    "    Build a MinHash signature from a set of shingles.\n",
    "    num_perm controls the signature size (and accuracy).\n",
    "    \"\"\"\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for sh in shingles:\n",
    "        m.update(sh.encode('utf-8'))\n",
    "    return m\n",
    "\n",
    "num_perm = 128\n",
    "k = 5  # shingle size\n",
    "\n",
    "minhashes = {}\n",
    "\n",
    "for idx, text in news_articles['text'].items():\n",
    "    shingles = get_shingles(text, k=k)\n",
    "    m = create_minhash(shingles, num_perm=num_perm)\n",
    "    minhashes[idx] = m\n",
    "\n",
    "print(\"Created MinHash signatures for\", len(minhashes), \"articles.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EnhancedLSH:\n",
    "    \"\"\"\n",
    "    LSH implementation with multiple hash families and analysis tools.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.5, num_perm=128, num_bands=None):\n",
    "        self.threshold = threshold\n",
    "        self.num_perm = num_perm\n",
    "        self.num_bands = num_bands or max(1, num_perm // 8)\n",
    "        self.lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "        self.minhashes = {}\n",
    "        self.metadata = {}\n",
    "        \n",
    "    def add_document(self, doc_id, text, metadata=None):\n",
    "        \"\"\"\n",
    "        Add a document to the LSH index.\n",
    "        \"\"\"\n",
    "        shingles = get_shingles(text, k=5)\n",
    "        minhash = create_minhash(shingles, self.num_perm)\n",
    "        \n",
    "        self.lsh.insert(str(doc_id), minhash)\n",
    "        self.minhashes[doc_id] = minhash\n",
    "        \n",
    "        if metadata:\n",
    "            self.metadata[doc_id] = metadata\n",
    "    \n",
    "    def query_similar(self, doc_id, return_scores=True):\n",
    "        \"\"\"\n",
    "        Find similar documents to a given document.\n",
    "        \"\"\"\n",
    "        if doc_id not in self.minhashes:\n",
    "            return []\n",
    "            \n",
    "        query_hash = self.minhashes[doc_id]\n",
    "        candidates = self.lsh.query(query_hash)\n",
    "        \n",
    "        results = []\n",
    "        for candidate in candidates:\n",
    "            candidate_id = int(candidate)\n",
    "            if candidate_id == doc_id:\n",
    "                continue\n",
    "                \n",
    "            if return_scores:\n",
    "                similarity = query_hash.jaccard(self.minhashes[candidate_id])\n",
    "                results.append((candidate_id, similarity))\n",
    "            else:\n",
    "                results.append(candidate_id)\n",
    "        \n",
    "        if return_scores:\n",
    "            results.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def get_all_similar_pairs(self):\n",
    "        \"\"\"\n",
    "        Get all similar pairs found by LSH.\n",
    "        \"\"\"\n",
    "        similar_pairs = set()\n",
    "        \n",
    "        for doc_id in self.minhashes:\n",
    "            neighbors = self.query_similar(doc_id, return_scores=False)\n",
    "            for neighbor_id in neighbors:\n",
    "                pair = tuple(sorted((doc_id, neighbor_id)))\n",
    "                similar_pairs.add(pair)\n",
    "        \n",
    "        return similar_pairs\n",
    "\n",
    "# LSH index\n",
    "enhanced_lsh = EnhancedLSH(threshold=0.4, num_perm=128)\n",
    "\n",
    "for idx, row in news_articles.iterrows():\n",
    "    metadata = {\n",
    "        'country': row['country'],\n",
    "        'source': row['source'],\n",
    "        'worldview_score': row['worldview_score'],\n",
    "        'headline': row['headline']\n",
    "    }\n",
    "    enhanced_lsh.add_document(idx, row['text'], metadata)\n",
    "\n",
    "print(f\"LSH index built with {len(enhanced_lsh.minhashes)} documents\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### LSH Performance\n",
    "\n",
    "Given an `EnhancedLSH` instance, it:\n",
    "\n",
    "1. Retrieves all similar article pairs that LSH has found.\n",
    "2. Computes the true MinHash Jaccard similarity for each pair.\n",
    "3. Tracks how often similar pairs share:\n",
    "   - the same ***country***,\n",
    "   - the same ***source***,\n",
    "   - the same ***worldview score***."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_lsh_performance(enhanced_lsh, news_df):\n",
    "    \"\"\"\n",
    "    Analyze LSH performance and similarity patterns.\n",
    "    \"\"\"\n",
    "    all_pairs = enhanced_lsh.get_all_similar_pairs()\n",
    "    \n",
    "    print(f\"Total similar pairs found: {len(all_pairs)}\")\n",
    "    \n",
    "    # Analyze similarity distribution\n",
    "    similarities = []\n",
    "    country_matches = 0\n",
    "    source_matches = 0\n",
    "    worldview_matches = 0\n",
    "    \n",
    "    for doc1, doc2 in all_pairs:\n",
    "        sim = enhanced_lsh.minhashes[doc1].jaccard(enhanced_lsh.minhashes[doc2])\n",
    "        similarities.append(sim)\n",
    "        \n",
    "        meta1 = enhanced_lsh.metadata[doc1]\n",
    "        meta2 = enhanced_lsh.metadata[doc2]\n",
    "        \n",
    "        if meta1['country'] == meta2['country']:\n",
    "            country_matches += 1\n",
    "        if meta1['source'] == meta2['source']:\n",
    "            source_matches += 1\n",
    "        if meta1['worldview_score'] == meta2['worldview_score']:\n",
    "            worldview_matches += 1\n",
    "    \n",
    "    if len(all_pairs) > 0:\n",
    "        print(f\"Average similarity: {np.mean(similarities):.3f}\")\n",
    "        print(f\"Same country ratio: {country_matches / len(all_pairs):.3f}\")\n",
    "        print(f\"Same source ratio: {source_matches / len(all_pairs):.3f}\")\n",
    "        print(f\"Same worldview ratio: {worldview_matches / len(all_pairs):.3f}\")\n",
    "    \n",
    "    return similarities, all_pairs\n",
    "\n",
    "similarities, similar_pairs = analyze_lsh_performance(enhanced_lsh, news_articles)\n",
    "\n",
    "# Show top similar pairs\n",
    "pair_details = []\n",
    "for doc1, doc2 in list(similar_pairs)[:10]:\n",
    "    sim = enhanced_lsh.minhashes[doc1].jaccard(enhanced_lsh.minhashes[doc2])\n",
    "    pair_details.append((sim, doc1, doc2))\n",
    "\n",
    "pair_details.sort(reverse=True)\n",
    "\n",
    "print(\"\\nTop 3 most similar pairs:\")\n",
    "for sim, doc1, doc2 in pair_details[:3]:\n",
    "    print(f\"\\nSimilarity: {sim:.3f} | Pair: {doc1} – {doc2}\")\n",
    "    print(f\"A: [{news_articles.loc[doc1, 'source']}] {news_articles.loc[doc1, 'headline']}\"\n",
    "          f\"\\nContent: {news_articles.loc[doc1, 'content_clean'][:150]}\")\n",
    "    print(f\"B: [{news_articles.loc[doc2, 'source']}] {news_articles.loc[doc2, 'headline']}\"\n",
    "        f\"\\nContent: {news_articles.loc[doc2, 'content_clean'][:150]}\") "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To compare different LSH similarity thresholds, the LSH pipeline runs multiple times with varying `threshold` values.\n",
    "\n",
    "The table reveals what threshold that balances: \n",
    "- *coverage* (not miss interesting similar articles),\n",
    "- and *precision* (avoid flooding the analysis with weak matches).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compare_lsh_thresholds(thresholds, news_df):\n",
    "    \"\"\"\n",
    "    Compare LSH performance across different similarity thresholds.\n",
    "    \"\"\"\n",
    "    results_summary = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        lsh_test = EnhancedLSH(threshold=threshold, num_perm=128)\n",
    "        \n",
    "        for idx, row in news_df.iterrows():\n",
    "            metadata = {\n",
    "                'country': row['country'],\n",
    "                'source': row['source'],\n",
    "                'worldview_score': row['worldview_score']\n",
    "            }\n",
    "            lsh_test.add_document(idx, row['text'], metadata)\n",
    "        \n",
    "        pairs = lsh_test.get_all_similar_pairs()\n",
    "        \n",
    "        if len(pairs) > 0:\n",
    "            sims, _ = analyze_lsh_performance(lsh_test, news_df)\n",
    "            avg_sim = np.mean(sims)\n",
    "        else:\n",
    "            avg_sim = 0\n",
    "        \n",
    "        results_summary.append({\n",
    "            'threshold': threshold,\n",
    "            'n_pairs': len(pairs),\n",
    "            'avg_similarity': avg_sim\n",
    "        })\n",
    "            \n",
    "    return pd.DataFrame(results_summary)\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "threshold_results = compare_lsh_thresholds(thresholds, news_articles)\n",
    "\n",
    "display(HTML(\"<h4>LSH Threshold Comparison</h4>\"))\n",
    "display(HTML(threshold_results.to_html(index=False)))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSH vs ANN Comparison"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compare_ann_vs_lsh(query_idx, ann_index, enhanced_lsh, matrix, news_df, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    Compare ANN and LSH results for a specific query article.\n",
    "    \"\"\"\n",
    "    # ANN results\n",
    "    ann_indices, ann_distances = find_similar_articles(query_idx, ann_index, matrix, n_neighbors)\n",
    "    ann_similarities = 1 - ann_distances\n",
    "    \n",
    "    # LSH results\n",
    "    lsh_results = enhanced_lsh.query_similar(query_idx, return_scores=True)\n",
    "    lsh_indices = [idx for idx, _ in lsh_results]\n",
    "    lsh_similarities = [sim for _, sim in lsh_results]\n",
    "    \n",
    "    print(f\"Query article {query_idx}: '{news_df.loc[query_idx, 'headline']}'\")\n",
    "    print(f\"Country: {news_df.loc[query_idx, 'country']} | Source: {news_df.loc[query_idx, 'source']}\\n\")\n",
    "    \n",
    "    print(f\"ANN found {len(ann_indices)} neighbors\")\n",
    "    print(f\"LSH found {len(lsh_indices)} neighbors\\n\")\n",
    "    \n",
    "    # Show overlap\n",
    "    ann_set = set(ann_indices)\n",
    "    lsh_set = set(lsh_indices)\n",
    "    overlap = ann_set.intersection(lsh_set)\n",
    "    \n",
    "    print(f\"Overlap: {len(overlap)} articles\")\n",
    "    print(f\"ANN-only: {len(ann_set - lsh_set)} articles\")\n",
    "    print(f\"LSH-only: {len(lsh_set - ann_set)} articles\\n\")\n",
    "    \n",
    "    # top 3 results\n",
    "    print(\"Top 3 ANN results:\")\n",
    "    for i, (idx, sim) in enumerate(zip(ann_indices[:3], ann_similarities[:3])):\n",
    "        print(f\"  {i+1}. Sim: {sim:.3f} | [{news_df.loc[idx, 'source']}] {news_df.loc[idx, 'headline']}\")\n",
    "    \n",
    "    print(\"\\nTop 3 LSH results:\")\n",
    "    for i, (idx, sim) in enumerate(lsh_results[:3]):\n",
    "        print(f\"  {i+1}. Sim: {sim:.3f} | [{news_df.loc[idx, 'source']}] {news_df.loc[idx, 'headline']}\")\n",
    "\n",
    "    if len(overlap) == 0:\n",
    "        print(\"\\nNo overlapping articles by LSH.\")\n",
    "\n",
    "    if len(overlap) > 0:\n",
    "        print(\"\\nOverlapping articles:\")\n",
    "        for idx in overlap:\n",
    "            print(f\"  [{news_df.loc[idx, 'source']}] {news_df.loc[idx, 'headline']}\")\n",
    "\n",
    "# compare with sampled article\n",
    "sample_article = 50\n",
    "\n",
    "print(\"=\" * 80)\n",
    "compare_ann_vs_lsh(sample_article, ann_index, enhanced_lsh, tfidf_matrix, news_articles, n_neighbors=5)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
